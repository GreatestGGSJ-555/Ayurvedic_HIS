{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40a9c9a2-af3b-4805-a952-6f750d42dcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = 'cuda' if t.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))\n",
    "print(device)\n",
    "\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "\n",
    "max_iters = 10000\n",
    "#eval interval = 2500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4a7bcb5-3b25-4917-bf31-083844a7f22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text =  748801\n",
      "Ayurveda, the Ancient Science of Hindus and Indians, dates back about 7000 years.\n",
      "It has eight branches, one of which is Rasayana Tantra. The word rasayana literally\n",
      "means the path that rasa takes (ra\n"
     ]
    }
   ],
   "source": [
    "with open(\"Rasayana_ Ayurvedic herbs for longevity and rejuvenation.txt\" , \"r\", encoding = 'utf-8' ) as f:\n",
    "    text = f.read()\n",
    "print(\"text = \", len(text))\n",
    "print(text[:200]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a3e1990-2a0c-4001-8322-f9740735af9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '\\x0c', ' ', '!', '&', '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '®', '°', 'µ', '×', 'à', 'é', 'α', 'β', 'χ', '–', '‘', '’', '“', '”', '−', 'ﬁ', 'ﬂ']\n",
      "97\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "print(len(chars)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "348a9f74-48e8-4252-b45e-61fafd27a744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([28, 78, 74, 71, 75, 58, 57, 54,  9,  2, 73, 61, 58,  2, 28, 67, 56, 62,\n",
      "        58, 67, 73,  2, 46, 56, 62, 58, 67, 56, 58,  2, 68, 59,  2, 35, 62, 67,\n",
      "        57, 74, 72,  2, 54, 67, 57,  2, 36, 67, 57, 62, 54, 67, 72,  9,  2, 57,\n",
      "        54, 73, 58, 72,  2, 55, 54, 56, 64,  2, 54, 55, 68, 74, 73,  2, 20, 13,\n",
      "        13, 13,  2, 78, 58, 54, 71, 72, 11,  0, 36, 73,  2, 61, 54, 72,  2, 58,\n",
      "        62, 60, 61, 73,  2, 55, 71, 54, 67, 56])\n"
     ]
    }
   ],
   "source": [
    "string_to_int = {ch:i for i, ch in enumerate(chars) }\n",
    "int_to_string = {i:ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data = t.tensor(encode(text), dtype = t.long)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef479650-7627-417c-9450-0ea7d8ff1c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded =  [61, 58, 65, 65, 68]\n",
      "Decoded =  hello\n"
     ]
    }
   ],
   "source": [
    "enc_hello = encode(\"hello\")\n",
    "dec_hello = decode(enc_hello)\n",
    "print(\"Encoded = \", enc_hello)\n",
    "print(\"Decoded = \", dec_hello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b3766a0-3ea0-40a5-bac1-9f18e36b4a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape =  torch.Size([599040])\n",
      "Test Shape =  torch.Size([149761])\n",
      "Inputs = \n",
      "tensor([[11,  2, 74, 71, 62, 67, 54, 71],\n",
      "        [66,  2, 69, 74, 72,  9,  2, 54],\n",
      "        [ 2, 73, 58, 77, 73, 72,  9,  2],\n",
      "        [67, 60,  2, 55, 71, 54, 67, 56]], device='cuda:0')\n",
      "Targets = \n",
      "tensor([[ 2, 74, 71, 62, 67, 54, 71, 62],\n",
      "        [ 2, 69, 74, 72,  9,  2, 54,  2],\n",
      "        [73, 58, 77, 73, 72,  9,  2, 58],\n",
      "        [60,  2, 55, 71, 54, 67, 56, 61]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "n =  int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "test_data = data[n:]\n",
    "print(\"Train Shape = \", train_data.shape)\n",
    "print(\"Test Shape = \", test_data.shape)\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else test_data\n",
    "    ix = t.randint(len(data) - block_size, (batch_size, ))\n",
    "    # print(ix)\n",
    "    x = t.stack([data[i:i+block_size] for i in ix])\n",
    "    y = t.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x,y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print(\"Inputs = \")\n",
    "print(x)\n",
    "print('Targets = ')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e9175de-492a-4949-836a-7d2c23a3bf31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the input is tensor([28]) target is tensor(78)\n",
      "When the input is tensor([28, 78]) target is tensor(74)\n",
      "When the input is tensor([28, 78, 74]) target is tensor(71)\n",
      "When the input is tensor([28, 78, 74, 71]) target is tensor(75)\n",
      "When the input is tensor([28, 78, 74, 71, 75]) target is tensor(58)\n",
      "When the input is tensor([28, 78, 74, 71, 75, 58]) target is tensor(57)\n",
      "When the input is tensor([28, 78, 74, 71, 75, 58, 57]) target is tensor(54)\n",
      "When the input is tensor([28, 78, 74, 71, 75, 58, 57, 54]) target is tensor(9)\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for io in range(block_size):\n",
    "    context = x[:io+1]\n",
    "    target = y[io]\n",
    "    print(\"When the input is\", context, \"target is\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72e62503",
   "metadata": {},
   "outputs": [],
   "source": [
    "@t.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train','val']:\n",
    "        losses  = t.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f678bef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">06cy−))zkfIχ\n",
      "AZgGo)rShQv-;ﬁ63×FGy;ﬂTp gG;χ)gE>J(−’e:G:(αﬁﬂβJf1bqµ®8àP!k)Z3–+)2 :b3&:.µERirgdagG-pBkjk&Np!ilGj+1Nχ9jKi(3@H4Uj.F(d9fdﬁS,χEβYS,β,Wx\fDs(L)e@4Qwﬁkf-iqmkχ5e“Kd*2W9χI>yQnH−oβ°-IeCG:Z°IG\f“*@qEoRSQ;ﬂ6w,C-FQrBa2Owr2K2‘L69GAe&Feem77FQQ,Hz–vCq3×,iTk((hm92o–vw6hVﬁ?FeHVﬁS,92+αb\fgG8KM(UjDAO6>h6:ag(g,χYO4Mf°cc89NAJaX;A-αéa/r*Fs 4yx5!3c“j3é‘V0–lnnkq:qef–QhmXG;1!b3d–r*AS)7:)hu\n",
      "z/ ﬂcPe:β\n",
      "5Dm(c,9\n",
      "5ﬂTcE>W4cKn‘β:@tG4co&T®tαGﬁ:ﬁs0DàχX®31’IK7>-;−vxβ:@Y(v\n",
      "9\n",
      "Pµaﬂl6”ﬂw8αvG\fTSBl;\n",
      "FtVw5V7Wα?;ﬂ\n",
      "°,β“K>c 9dﬁ5-@M&7>\fD1T:v5®p°5m+°WIOppxX 9vU Uatµ/3×yJ9àU :,\fYBBﬁbH(r4Pe6*µe;ﬂD@cMm”g4\f+®kGBq8βF(Gt®”U7,up?°gà°mPePL9Wﬂ+:u°B5GM(WF&o9p−/−ikUà0T–YEG×(OKαZl7>hplEβ\n",
      "1B:.p+Ef‘βm8k“w5 f×RObp\n",
      "zgOMYV-jW×+q−3pB04A-H®q–u!r9×s(h3’W4?*0Z) uM5‘:aa-µαE>T”–rﬁzp–V–‘d\n",
      "MIP(1é8vMfstAPp−j.aaRT*cl\f“4/(GKàEkB u\n",
      "×\n",
      "1’®aU9yJ@b)χﬂ&×6iT:kGelxjVIer;y:Cn(W>b°Dqch@−qµg4C! a ﬂ@z‘–tSF’Kn!‘*c°XVmχ’é–hf–Gﬁ0j(1àh?WVh6>nW-(lα)r/:àS4XVPfo+®Ag:3(’éj−−x*f&f°*LyYZOBNA*)AVqW@q,D51‘‘Wq&Kvm8tRAH(°O4n!/χNcg6*6PFQY&d/Tp−−Mf8CβYEbHfTχ‘@bV\n",
      "8ZltqDn!kaBU2L\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModule(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # creates a matrix of size (vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, index, target = None):\n",
    "        #logits are normalized floating point numbers that are the input to the softmax function\n",
    "        logits = self.token_embedding_table(index)\n",
    "        \n",
    "        if target is None:\n",
    "            loss = None\n",
    "        else:    \n",
    "            #Bash, Time, Channels = vocab size\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            target = target.view(B*T)\n",
    "            loss = F.cross_entropy(logits, target)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        #index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions \n",
    "            logits, loss = self.forward(index)\n",
    "            # focus on the last time step\n",
    "            logits = logits[:, -1, :] # becomes(B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim = -1) # focous on last dimension\n",
    "            # sample from the distribution to get new token\n",
    "            index_next = t.multinomial(probs, num_samples = 1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            index = t.cat((index, index_next), dim = 1) # (B, T+1)\n",
    "                \n",
    "        return index\n",
    "    \n",
    "model = BigramLanguageModule(vocab_size = len(chars))\n",
    "m = model.to(device)\n",
    "    \n",
    "context = t.zeros((1,1), dtype = t.long, device = device) \n",
    "generated_chars = decode(m.generate(context, max_new_tokens = 1000)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1996cabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "βLKI®3,X−I2χ>BTOAOQ+Ec‘βMfNQCn®9®µt!Nf+65\fuMpα7:–vyl&W?K‘,α”P”g:µµhlχwxy–S,HﬁZ;e\n",
      "lwuRRG-µOo)nx+®oK5vQwALL\n",
      "\n",
      "zQ\fP*2AQ>’pTu°méo+9-Ovi’”j\n",
      ">D7vﬂY(méAr*FtnMLOFes4*i65mEµ4va3j0NFPµOe;wβ+®ﬁfY(,β:TﬂJl,µCfnS192 +α7l2W+)eK×µ–QYX;ﬁsthV l@zY°C×Rm”7Js4,Is‘kSAZ®u0\fJ?–oX!/w5 Qvixjc‘YαU,:DP7:xy”–.W 95–izpLkt®DP–5(1y2)JzpRIHWI\fZT:-:+8o\f(>Q;brN−HnGCfTtnRAGM×m.lBMféHhG)HTpnSQﬁ−µβJ(LK×9×U7βuM(°rO\f(>W‘TQYBmkD7J’wrb\n",
      "°UT@oU>βfà*dﬁC1*02.MχU *!s) 2F0àvMU9G- –o)/09@bGH‘NZR-53×X TTw“*Hχ>1KàJqA2Z!?5β:uMn’Wgµe;Jg”B‘*8–wg:WZ>Bl2QO”×*cilx5 \fL4Pàd1χ>’jà.YTµ–bka5RNHz“8‘és5w1*lnAéPC-a-E/i)+8>s−:&b”:*T*L1\n",
      "Q(Qt’X3DOQwrαvF5*dﬁRNαz@lχ°”–6b/nR7D(Sαl–fC×fqB”ct>gLn!oop&K:25bHH>B@b&USP−tz”EàxAé>,@a”Sx:)eU3ﬂ”ﬂ ﬂµO’4498‘2)i×-2/;hmS>bI7u0°P7\fFM9–o7b)χY5m”,?R®r4yJ7χs\fG®hy,xYO−sàﬂ“LZl&&–f–”wRGVU,fHχ7J7\fhjf×1u\n",
      "qα0\n",
      "n+TQ+s®α!–w,6c“(bPlY\n",
      ">W°Mv!ﬂrSURLPLA×R®jβOnPa-Cb>h(EuWFRtSwββXgχﬂ+ hµ:cdP@F(Se\n",
      "iµaD7W4.hAgµg,®lχ79izqα\f2U-s–*×g:2Oa.J−>\n",
      "QN*9;ypwMeF8U.dﬁ’p ﬂA5;–(h:W’.E+-CgGMf*be*\n",
      "FvBT:kaW×(“Lytna/VwVN*.l&h’F!6s®8−@eO>buCW6°−s®0O\f6”cu°−TX8U\n"
     ]
    }
   ],
   "source": [
    "context = t.zeros((1,1), dtype = t.long, device = device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens = 1000)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41729a42",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-box alert-success\">\n",
    "    <h5>The above is the code for bigram which is now running well and now needs to be optimizeed</h5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6964ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 4.9244, val loss: 4.9931\n",
      "step: 250, train loss: 4.8481, val loss: 4.9567\n",
      "step: 500, train loss: 4.7777, val loss: 4.8650\n",
      "step: 750, train loss: 4.7597, val loss: 4.8312\n",
      "step: 1000, train loss: 4.6743, val loss: 4.7666\n",
      "step: 1250, train loss: 4.6201, val loss: 4.7051\n",
      "step: 1500, train loss: 4.5615, val loss: 4.6355\n",
      "step: 1750, train loss: 4.5139, val loss: 4.6033\n",
      "step: 2000, train loss: 4.4591, val loss: 4.5775\n",
      "step: 2250, train loss: 4.4207, val loss: 4.5159\n",
      "step: 2500, train loss: 4.3830, val loss: 4.4725\n",
      "step: 2750, train loss: 4.3270, val loss: 4.4201\n",
      "step: 3000, train loss: 4.2515, val loss: 4.3759\n",
      "step: 3250, train loss: 4.2323, val loss: 4.3244\n",
      "step: 3500, train loss: 4.1886, val loss: 4.2774\n",
      "step: 3750, train loss: 4.1572, val loss: 4.2371\n",
      "step: 4000, train loss: 4.0967, val loss: 4.1780\n",
      "step: 4250, train loss: 4.0600, val loss: 4.1575\n",
      "step: 4500, train loss: 4.0208, val loss: 4.1151\n",
      "step: 4750, train loss: 3.9907, val loss: 4.0546\n",
      "step: 5000, train loss: 3.9547, val loss: 4.0392\n",
      "step: 5250, train loss: 3.9118, val loss: 3.9889\n",
      "step: 5500, train loss: 3.8597, val loss: 3.9481\n",
      "step: 5750, train loss: 3.8191, val loss: 3.9179\n",
      "step: 6000, train loss: 3.8047, val loss: 3.8873\n",
      "step: 6250, train loss: 3.7471, val loss: 3.8207\n",
      "step: 6500, train loss: 3.7102, val loss: 3.8086\n",
      "step: 6750, train loss: 3.6925, val loss: 3.7831\n",
      "step: 7000, train loss: 3.6474, val loss: 3.7474\n",
      "step: 7250, train loss: 3.6329, val loss: 3.7022\n",
      "step: 7500, train loss: 3.6037, val loss: 3.6881\n",
      "step: 7750, train loss: 3.6142, val loss: 3.6520\n",
      "step: 8000, train loss: 3.5300, val loss: 3.6253\n",
      "step: 8250, train loss: 3.5169, val loss: 3.6032\n",
      "step: 8500, train loss: 3.4957, val loss: 3.5759\n",
      "step: 8750, train loss: 3.4732, val loss: 3.5449\n",
      "step: 9000, train loss: 3.4299, val loss: 3.5126\n",
      "step: 9250, train loss: 3.4260, val loss: 3.5114\n",
      "step: 9500, train loss: 3.4029, val loss: 3.4991\n",
      "step: 9750, train loss: 3.3983, val loss: 3.4509\n",
      "step: 10000, train loss: 3.3565, val loss: 3.4237\n",
      "step: 10250, train loss: 3.3342, val loss: 3.3914\n",
      "step: 10500, train loss: 3.3168, val loss: 3.3425\n",
      "step: 10750, train loss: 3.3324, val loss: 3.3458\n",
      "step: 11000, train loss: 3.2987, val loss: 3.3241\n",
      "step: 11250, train loss: 3.2599, val loss: 3.3254\n",
      "step: 11500, train loss: 3.2545, val loss: 3.2876\n",
      "step: 11750, train loss: 3.2110, val loss: 3.2836\n",
      "step: 12000, train loss: 3.2232, val loss: 3.2375\n",
      "step: 12250, train loss: 3.1773, val loss: 3.2423\n",
      "step: 12500, train loss: 3.1690, val loss: 3.2116\n",
      "step: 12750, train loss: 3.1595, val loss: 3.2031\n",
      "step: 13000, train loss: 3.1563, val loss: 3.1674\n",
      "step: 13250, train loss: 3.1220, val loss: 3.1444\n",
      "step: 13500, train loss: 3.1015, val loss: 3.1433\n",
      "step: 13750, train loss: 3.0875, val loss: 3.1328\n",
      "step: 14000, train loss: 3.0659, val loss: 3.1092\n",
      "step: 14250, train loss: 3.0641, val loss: 3.0835\n",
      "step: 14500, train loss: 3.0503, val loss: 3.0726\n",
      "step: 14750, train loss: 3.0389, val loss: 3.0519\n",
      "step: 15000, train loss: 3.0306, val loss: 3.0513\n",
      "step: 15250, train loss: 3.0101, val loss: 3.0245\n",
      "step: 15500, train loss: 2.9857, val loss: 3.0167\n",
      "step: 15750, train loss: 2.9796, val loss: 3.0032\n",
      "step: 16000, train loss: 2.9818, val loss: 3.0043\n",
      "step: 16250, train loss: 2.9764, val loss: 2.9848\n",
      "step: 16500, train loss: 2.9506, val loss: 2.9564\n",
      "step: 16750, train loss: 2.9574, val loss: 2.9419\n",
      "step: 17000, train loss: 2.9712, val loss: 2.9566\n",
      "step: 17250, train loss: 2.9319, val loss: 2.9215\n",
      "step: 17500, train loss: 2.9308, val loss: 2.9391\n",
      "step: 17750, train loss: 2.9110, val loss: 2.9235\n",
      "step: 18000, train loss: 2.9092, val loss: 2.8922\n",
      "step: 18250, train loss: 2.9023, val loss: 2.8958\n",
      "step: 18500, train loss: 2.8773, val loss: 2.8961\n",
      "step: 18750, train loss: 2.9083, val loss: 2.8704\n",
      "step: 19000, train loss: 2.8820, val loss: 2.8607\n",
      "step: 19250, train loss: 2.8979, val loss: 2.8451\n",
      "step: 19500, train loss: 2.8569, val loss: 2.8606\n",
      "step: 19750, train loss: 2.8267, val loss: 2.8469\n",
      "step: 20000, train loss: 2.8582, val loss: 2.8582\n",
      "step: 20250, train loss: 2.8375, val loss: 2.8189\n",
      "step: 20500, train loss: 2.8652, val loss: 2.8057\n",
      "step: 20750, train loss: 2.8359, val loss: 2.8138\n",
      "step: 21000, train loss: 2.8258, val loss: 2.7946\n",
      "step: 21250, train loss: 2.8203, val loss: 2.7864\n",
      "step: 21500, train loss: 2.8397, val loss: 2.7950\n",
      "step: 21750, train loss: 2.8088, val loss: 2.7806\n",
      "step: 22000, train loss: 2.8289, val loss: 2.7791\n",
      "step: 22250, train loss: 2.7974, val loss: 2.7665\n",
      "step: 22500, train loss: 2.8227, val loss: 2.7733\n",
      "step: 22750, train loss: 2.7909, val loss: 2.7596\n",
      "step: 23000, train loss: 2.7730, val loss: 2.7464\n",
      "step: 23250, train loss: 2.7795, val loss: 2.7361\n",
      "step: 23500, train loss: 2.7728, val loss: 2.7308\n",
      "step: 23750, train loss: 2.7666, val loss: 2.7253\n",
      "step: 24000, train loss: 2.7817, val loss: 2.7315\n",
      "step: 24250, train loss: 2.7606, val loss: 2.7163\n",
      "step: 24500, train loss: 2.7699, val loss: 2.6965\n",
      "step: 24750, train loss: 2.7507, val loss: 2.6872\n",
      "step: 25000, train loss: 2.7532, val loss: 2.7088\n",
      "step: 25250, train loss: 2.7462, val loss: 2.6892\n",
      "step: 25500, train loss: 2.7551, val loss: 2.6961\n",
      "step: 25750, train loss: 2.7540, val loss: 2.6552\n",
      "step: 26000, train loss: 2.7457, val loss: 2.6673\n",
      "step: 26250, train loss: 2.7239, val loss: 2.6615\n",
      "step: 26500, train loss: 2.7292, val loss: 2.6832\n",
      "step: 26750, train loss: 2.7369, val loss: 2.6566\n",
      "step: 27000, train loss: 2.7455, val loss: 2.6421\n",
      "step: 27250, train loss: 2.7317, val loss: 2.6505\n",
      "step: 27500, train loss: 2.7210, val loss: 2.6480\n",
      "step: 27750, train loss: 2.7317, val loss: 2.6378\n",
      "step: 28000, train loss: 2.7210, val loss: 2.6349\n",
      "step: 28250, train loss: 2.7213, val loss: 2.6350\n",
      "step: 28500, train loss: 2.7267, val loss: 2.6144\n",
      "step: 28750, train loss: 2.6915, val loss: 2.6436\n",
      "step: 29000, train loss: 2.7021, val loss: 2.6377\n",
      "step: 29250, train loss: 2.7209, val loss: 2.6314\n",
      "step: 29500, train loss: 2.7001, val loss: 2.6489\n",
      "step: 29750, train loss: 2.7070, val loss: 2.6293\n",
      "step: 30000, train loss: 2.7098, val loss: 2.6114\n",
      "step: 30250, train loss: 2.7044, val loss: 2.6098\n",
      "step: 30500, train loss: 2.6982, val loss: 2.6099\n",
      "step: 30750, train loss: 2.6946, val loss: 2.5813\n",
      "step: 31000, train loss: 2.7178, val loss: 2.6456\n",
      "step: 31250, train loss: 2.7178, val loss: 2.5589\n",
      "step: 31500, train loss: 2.6989, val loss: 2.5856\n",
      "step: 31750, train loss: 2.7020, val loss: 2.5909\n",
      "step: 32000, train loss: 2.6969, val loss: 2.5977\n",
      "step: 32250, train loss: 2.6891, val loss: 2.5840\n",
      "step: 32500, train loss: 2.6867, val loss: 2.5894\n",
      "step: 32750, train loss: 2.6838, val loss: 2.5840\n",
      "step: 33000, train loss: 2.6583, val loss: 2.5816\n",
      "step: 33250, train loss: 2.6660, val loss: 2.5586\n",
      "step: 33500, train loss: 2.6839, val loss: 2.5576\n",
      "step: 33750, train loss: 2.6567, val loss: 2.5658\n",
      "step: 34000, train loss: 2.6617, val loss: 2.5756\n",
      "step: 34250, train loss: 2.6912, val loss: 2.5530\n",
      "step: 34500, train loss: 2.6669, val loss: 2.5641\n",
      "step: 34750, train loss: 2.6443, val loss: 2.5667\n",
      "step: 35000, train loss: 2.6573, val loss: 2.5793\n",
      "step: 35250, train loss: 2.6600, val loss: 2.5782\n",
      "step: 35500, train loss: 2.6810, val loss: 2.5406\n",
      "step: 35750, train loss: 2.6803, val loss: 2.5377\n",
      "step: 36000, train loss: 2.6464, val loss: 2.5701\n",
      "step: 36250, train loss: 2.6589, val loss: 2.5517\n",
      "step: 36500, train loss: 2.6487, val loss: 2.5686\n",
      "step: 36750, train loss: 2.6432, val loss: 2.5465\n",
      "step: 37000, train loss: 2.6446, val loss: 2.5523\n",
      "step: 37250, train loss: 2.6624, val loss: 2.5513\n",
      "step: 37500, train loss: 2.6564, val loss: 2.5499\n",
      "step: 37750, train loss: 2.6461, val loss: 2.5219\n",
      "step: 38000, train loss: 2.6604, val loss: 2.5411\n",
      "step: 38250, train loss: 2.6567, val loss: 2.5420\n",
      "step: 38500, train loss: 2.6594, val loss: 2.5491\n",
      "step: 38750, train loss: 2.6530, val loss: 2.5253\n",
      "step: 39000, train loss: 2.6564, val loss: 2.5491\n",
      "step: 39250, train loss: 2.6323, val loss: 2.5488\n",
      "step: 39500, train loss: 2.6646, val loss: 2.5621\n",
      "step: 39750, train loss: 2.6523, val loss: 2.5442\n",
      "step: 40000, train loss: 2.6538, val loss: 2.5479\n",
      "step: 40250, train loss: 2.6555, val loss: 2.5331\n",
      "step: 40500, train loss: 2.6345, val loss: 2.5216\n",
      "step: 40750, train loss: 2.6448, val loss: 2.5370\n",
      "step: 41000, train loss: 2.6449, val loss: 2.5168\n",
      "step: 41250, train loss: 2.6475, val loss: 2.5176\n",
      "step: 41500, train loss: 2.6630, val loss: 2.5259\n",
      "step: 41750, train loss: 2.6446, val loss: 2.5333\n",
      "step: 42000, train loss: 2.6411, val loss: 2.5330\n",
      "step: 42250, train loss: 2.6442, val loss: 2.5309\n",
      "step: 42500, train loss: 2.6294, val loss: 2.5176\n",
      "step: 42750, train loss: 2.6311, val loss: 2.5242\n",
      "step: 43000, train loss: 2.6214, val loss: 2.5032\n",
      "step: 43250, train loss: 2.6163, val loss: 2.5392\n",
      "step: 43500, train loss: 2.6277, val loss: 2.5317\n",
      "step: 43750, train loss: 2.6440, val loss: 2.5428\n",
      "step: 44000, train loss: 2.6293, val loss: 2.5239\n",
      "step: 44250, train loss: 2.6572, val loss: 2.5471\n",
      "step: 44500, train loss: 2.6443, val loss: 2.5404\n",
      "step: 44750, train loss: 2.6426, val loss: 2.5283\n",
      "step: 45000, train loss: 2.6233, val loss: 2.5087\n",
      "step: 45250, train loss: 2.6405, val loss: 2.4899\n",
      "step: 45500, train loss: 2.6599, val loss: 2.5217\n",
      "step: 45750, train loss: 2.6453, val loss: 2.5176\n",
      "step: 46000, train loss: 2.6264, val loss: 2.5090\n",
      "step: 46250, train loss: 2.6226, val loss: 2.5388\n",
      "step: 46500, train loss: 2.6193, val loss: 2.5182\n",
      "step: 46750, train loss: 2.6196, val loss: 2.5022\n",
      "step: 47000, train loss: 2.6319, val loss: 2.5345\n",
      "step: 47250, train loss: 2.6002, val loss: 2.5044\n",
      "step: 47500, train loss: 2.6282, val loss: 2.4970\n",
      "step: 47750, train loss: 2.6154, val loss: 2.5040\n",
      "step: 48000, train loss: 2.6235, val loss: 2.5460\n",
      "step: 48250, train loss: 2.6066, val loss: 2.4935\n",
      "step: 48500, train loss: 2.6152, val loss: 2.4898\n",
      "step: 48750, train loss: 2.5965, val loss: 2.5013\n",
      "step: 49000, train loss: 2.6252, val loss: 2.4929\n",
      "step: 49250, train loss: 2.6085, val loss: 2.4825\n",
      "step: 49500, train loss: 2.6057, val loss: 2.4861\n",
      "step: 49750, train loss: 2.6109, val loss: 2.4805\n",
      "step: 50000, train loss: 2.6234, val loss: 2.5141\n",
      "step: 50250, train loss: 2.6335, val loss: 2.5082\n",
      "step: 50500, train loss: 2.6213, val loss: 2.5086\n",
      "step: 50750, train loss: 2.6272, val loss: 2.5091\n",
      "step: 51000, train loss: 2.6274, val loss: 2.4939\n",
      "step: 51250, train loss: 2.6030, val loss: 2.4886\n",
      "step: 51500, train loss: 2.6035, val loss: 2.4744\n",
      "step: 51750, train loss: 2.6279, val loss: 2.5151\n",
      "step: 52000, train loss: 2.6056, val loss: 2.4704\n",
      "step: 52250, train loss: 2.6022, val loss: 2.4898\n",
      "step: 52500, train loss: 2.6139, val loss: 2.5005\n",
      "step: 52750, train loss: 2.6182, val loss: 2.4796\n",
      "step: 53000, train loss: 2.6212, val loss: 2.5071\n",
      "step: 53250, train loss: 2.5966, val loss: 2.4988\n",
      "step: 53500, train loss: 2.6230, val loss: 2.5135\n",
      "step: 53750, train loss: 2.6119, val loss: 2.5004\n",
      "step: 54000, train loss: 2.6218, val loss: 2.4844\n",
      "step: 54250, train loss: 2.6207, val loss: 2.4883\n",
      "step: 54500, train loss: 2.6351, val loss: 2.4931\n",
      "step: 54750, train loss: 2.6153, val loss: 2.4915\n",
      "step: 55000, train loss: 2.6006, val loss: 2.5056\n",
      "step: 55250, train loss: 2.6051, val loss: 2.5095\n",
      "step: 55500, train loss: 2.6037, val loss: 2.4914\n",
      "step: 55750, train loss: 2.6014, val loss: 2.4851\n",
      "step: 56000, train loss: 2.6217, val loss: 2.4913\n",
      "step: 56250, train loss: 2.6112, val loss: 2.4862\n",
      "step: 56500, train loss: 2.6166, val loss: 2.4862\n",
      "step: 56750, train loss: 2.6441, val loss: 2.4666\n",
      "step: 57000, train loss: 2.6142, val loss: 2.4805\n",
      "step: 57250, train loss: 2.6011, val loss: 2.4791\n",
      "step: 57500, train loss: 2.5833, val loss: 2.4667\n",
      "step: 57750, train loss: 2.5981, val loss: 2.4939\n",
      "step: 58000, train loss: 2.5940, val loss: 2.4982\n",
      "step: 58250, train loss: 2.6082, val loss: 2.4975\n",
      "step: 58500, train loss: 2.6319, val loss: 2.4858\n",
      "step: 58750, train loss: 2.5946, val loss: 2.5120\n",
      "step: 59000, train loss: 2.5936, val loss: 2.4930\n",
      "step: 59250, train loss: 2.6072, val loss: 2.4904\n",
      "step: 59500, train loss: 2.6209, val loss: 2.4752\n",
      "step: 59750, train loss: 2.5944, val loss: 2.4925\n",
      "step: 60000, train loss: 2.6226, val loss: 2.4655\n",
      "step: 60250, train loss: 2.6107, val loss: 2.4829\n",
      "step: 60500, train loss: 2.6109, val loss: 2.4891\n",
      "step: 60750, train loss: 2.6228, val loss: 2.4805\n",
      "step: 61000, train loss: 2.6186, val loss: 2.4893\n",
      "step: 61250, train loss: 2.6038, val loss: 2.4874\n",
      "step: 61500, train loss: 2.6123, val loss: 2.4776\n",
      "step: 61750, train loss: 2.5982, val loss: 2.4872\n",
      "step: 62000, train loss: 2.6217, val loss: 2.4838\n",
      "step: 62250, train loss: 2.5951, val loss: 2.4917\n",
      "step: 62500, train loss: 2.5920, val loss: 2.4859\n",
      "step: 62750, train loss: 2.6107, val loss: 2.4590\n",
      "step: 63000, train loss: 2.5889, val loss: 2.4957\n",
      "step: 63250, train loss: 2.6139, val loss: 2.5019\n",
      "step: 63500, train loss: 2.6005, val loss: 2.5101\n",
      "step: 63750, train loss: 2.5978, val loss: 2.4892\n",
      "step: 64000, train loss: 2.6054, val loss: 2.4991\n",
      "step: 64250, train loss: 2.6158, val loss: 2.5037\n",
      "step: 64500, train loss: 2.5823, val loss: 2.4860\n",
      "step: 64750, train loss: 2.5934, val loss: 2.5033\n",
      "step: 65000, train loss: 2.6046, val loss: 2.4946\n",
      "step: 65250, train loss: 2.5765, val loss: 2.5059\n",
      "step: 65500, train loss: 2.5907, val loss: 2.4826\n",
      "step: 65750, train loss: 2.5805, val loss: 2.4684\n",
      "step: 66000, train loss: 2.6084, val loss: 2.4947\n",
      "step: 66250, train loss: 2.6085, val loss: 2.4486\n",
      "step: 66500, train loss: 2.6056, val loss: 2.5013\n",
      "step: 66750, train loss: 2.6081, val loss: 2.5210\n",
      "step: 67000, train loss: 2.5959, val loss: 2.5004\n",
      "step: 67250, train loss: 2.5892, val loss: 2.4865\n",
      "step: 67500, train loss: 2.6193, val loss: 2.4840\n",
      "step: 67750, train loss: 2.5921, val loss: 2.5173\n",
      "step: 68000, train loss: 2.6189, val loss: 2.4687\n",
      "step: 68250, train loss: 2.6019, val loss: 2.4838\n",
      "step: 68500, train loss: 2.6103, val loss: 2.5082\n",
      "step: 68750, train loss: 2.5953, val loss: 2.4698\n",
      "step: 69000, train loss: 2.6254, val loss: 2.5015\n",
      "step: 69250, train loss: 2.6004, val loss: 2.4978\n",
      "step: 69500, train loss: 2.5812, val loss: 2.5174\n",
      "step: 69750, train loss: 2.6254, val loss: 2.5190\n",
      "step: 70000, train loss: 2.6026, val loss: 2.4904\n",
      "step: 70250, train loss: 2.5995, val loss: 2.4846\n",
      "step: 70500, train loss: 2.5893, val loss: 2.4987\n",
      "step: 70750, train loss: 2.6136, val loss: 2.4877\n",
      "step: 71000, train loss: 2.6185, val loss: 2.4936\n",
      "step: 71250, train loss: 2.5996, val loss: 2.5006\n",
      "step: 71500, train loss: 2.6070, val loss: 2.4751\n",
      "step: 71750, train loss: 2.6072, val loss: 2.4686\n",
      "step: 72000, train loss: 2.6144, val loss: 2.5020\n",
      "step: 72250, train loss: 2.5990, val loss: 2.4689\n",
      "step: 72500, train loss: 2.5690, val loss: 2.4857\n",
      "step: 72750, train loss: 2.5859, val loss: 2.4772\n",
      "step: 73000, train loss: 2.5869, val loss: 2.4647\n",
      "step: 73250, train loss: 2.6014, val loss: 2.4980\n",
      "step: 73500, train loss: 2.5895, val loss: 2.4581\n",
      "step: 73750, train loss: 2.6029, val loss: 2.4905\n",
      "step: 74000, train loss: 2.5718, val loss: 2.4546\n",
      "step: 74250, train loss: 2.5907, val loss: 2.4884\n",
      "step: 74500, train loss: 2.5826, val loss: 2.4916\n",
      "step: 74750, train loss: 2.6081, val loss: 2.4806\n",
      "step: 75000, train loss: 2.6092, val loss: 2.5122\n",
      "step: 75250, train loss: 2.5997, val loss: 2.4832\n",
      "step: 75500, train loss: 2.6102, val loss: 2.4811\n",
      "step: 75750, train loss: 2.5807, val loss: 2.4943\n",
      "step: 76000, train loss: 2.5807, val loss: 2.4966\n",
      "step: 76250, train loss: 2.6049, val loss: 2.5107\n",
      "step: 76500, train loss: 2.5994, val loss: 2.4630\n",
      "step: 76750, train loss: 2.5759, val loss: 2.4898\n",
      "step: 77000, train loss: 2.5809, val loss: 2.4708\n",
      "step: 77250, train loss: 2.5932, val loss: 2.4957\n",
      "step: 77500, train loss: 2.6044, val loss: 2.4880\n",
      "step: 77750, train loss: 2.6030, val loss: 2.5079\n",
      "step: 78000, train loss: 2.6073, val loss: 2.4885\n",
      "step: 78250, train loss: 2.5850, val loss: 2.5004\n",
      "step: 78500, train loss: 2.5992, val loss: 2.4787\n",
      "step: 78750, train loss: 2.5732, val loss: 2.5062\n",
      "step: 79000, train loss: 2.6007, val loss: 2.5272\n",
      "step: 79250, train loss: 2.5827, val loss: 2.4819\n",
      "step: 79500, train loss: 2.5939, val loss: 2.4838\n",
      "step: 79750, train loss: 2.6146, val loss: 2.4805\n",
      "step: 80000, train loss: 2.5996, val loss: 2.5018\n",
      "step: 80250, train loss: 2.5759, val loss: 2.4953\n",
      "step: 80500, train loss: 2.5852, val loss: 2.4651\n",
      "step: 80750, train loss: 2.5962, val loss: 2.4666\n",
      "step: 81000, train loss: 2.5902, val loss: 2.4825\n",
      "step: 81250, train loss: 2.5921, val loss: 2.4927\n",
      "step: 81500, train loss: 2.5813, val loss: 2.4915\n",
      "step: 81750, train loss: 2.5852, val loss: 2.4872\n",
      "step: 82000, train loss: 2.5950, val loss: 2.4548\n",
      "step: 82250, train loss: 2.5979, val loss: 2.4843\n",
      "step: 82500, train loss: 2.5838, val loss: 2.4825\n",
      "step: 82750, train loss: 2.5631, val loss: 2.4854\n",
      "step: 83000, train loss: 2.5971, val loss: 2.4874\n",
      "step: 83250, train loss: 2.5863, val loss: 2.4571\n",
      "step: 83500, train loss: 2.5971, val loss: 2.4738\n",
      "step: 83750, train loss: 2.6220, val loss: 2.4649\n",
      "step: 84000, train loss: 2.5906, val loss: 2.4913\n",
      "step: 84250, train loss: 2.5847, val loss: 2.4855\n",
      "step: 84500, train loss: 2.5857, val loss: 2.4928\n",
      "step: 84750, train loss: 2.5735, val loss: 2.4780\n",
      "step: 85000, train loss: 2.5857, val loss: 2.4956\n",
      "step: 85250, train loss: 2.5884, val loss: 2.4969\n",
      "step: 85500, train loss: 2.5793, val loss: 2.4622\n",
      "step: 85750, train loss: 2.5659, val loss: 2.4836\n",
      "step: 86000, train loss: 2.5975, val loss: 2.5136\n",
      "step: 86250, train loss: 2.5834, val loss: 2.4828\n",
      "step: 86500, train loss: 2.5958, val loss: 2.4698\n",
      "step: 86750, train loss: 2.6070, val loss: 2.4983\n",
      "step: 87000, train loss: 2.6043, val loss: 2.4622\n",
      "step: 87250, train loss: 2.6089, val loss: 2.4867\n",
      "step: 87500, train loss: 2.5820, val loss: 2.4586\n",
      "step: 87750, train loss: 2.6062, val loss: 2.4971\n",
      "step: 88000, train loss: 2.5899, val loss: 2.4616\n",
      "step: 88250, train loss: 2.6060, val loss: 2.4782\n",
      "step: 88500, train loss: 2.5842, val loss: 2.4787\n",
      "step: 88750, train loss: 2.5729, val loss: 2.4497\n",
      "step: 89000, train loss: 2.6052, val loss: 2.4908\n",
      "step: 89250, train loss: 2.5869, val loss: 2.4827\n",
      "step: 89500, train loss: 2.5866, val loss: 2.4659\n",
      "step: 89750, train loss: 2.5848, val loss: 2.4872\n",
      "step: 90000, train loss: 2.5910, val loss: 2.4706\n",
      "step: 90250, train loss: 2.5854, val loss: 2.4759\n",
      "step: 90500, train loss: 2.5792, val loss: 2.4825\n",
      "step: 90750, train loss: 2.5991, val loss: 2.4685\n",
      "step: 91000, train loss: 2.6061, val loss: 2.4559\n",
      "step: 91250, train loss: 2.6055, val loss: 2.4694\n",
      "step: 91500, train loss: 2.5841, val loss: 2.4835\n",
      "step: 91750, train loss: 2.6066, val loss: 2.4902\n",
      "step: 92000, train loss: 2.5880, val loss: 2.4739\n",
      "step: 92250, train loss: 2.5924, val loss: 2.4669\n",
      "step: 92500, train loss: 2.5959, val loss: 2.4886\n",
      "step: 92750, train loss: 2.5665, val loss: 2.4741\n",
      "step: 93000, train loss: 2.5822, val loss: 2.4549\n",
      "step: 93250, train loss: 2.5811, val loss: 2.4720\n",
      "step: 93500, train loss: 2.5812, val loss: 2.4930\n",
      "step: 93750, train loss: 2.5948, val loss: 2.4724\n",
      "step: 94000, train loss: 2.5856, val loss: 2.4918\n",
      "step: 94250, train loss: 2.5938, val loss: 2.4782\n",
      "step: 94500, train loss: 2.6149, val loss: 2.4805\n",
      "step: 94750, train loss: 2.5772, val loss: 2.4663\n",
      "step: 95000, train loss: 2.5883, val loss: 2.4663\n",
      "step: 95250, train loss: 2.5646, val loss: 2.4827\n",
      "step: 95500, train loss: 2.5919, val loss: 2.4880\n",
      "step: 95750, train loss: 2.5931, val loss: 2.4501\n",
      "step: 96000, train loss: 2.5811, val loss: 2.4971\n",
      "step: 96250, train loss: 2.5991, val loss: 2.4879\n",
      "step: 96500, train loss: 2.5901, val loss: 2.4472\n",
      "step: 96750, train loss: 2.5822, val loss: 2.4693\n",
      "step: 97000, train loss: 2.5753, val loss: 2.4891\n",
      "step: 97250, train loss: 2.6032, val loss: 2.4670\n",
      "step: 97500, train loss: 2.5843, val loss: 2.4746\n",
      "step: 97750, train loss: 2.5808, val loss: 2.4787\n",
      "step: 98000, train loss: 2.6065, val loss: 2.4888\n",
      "step: 98250, train loss: 2.5982, val loss: 2.4901\n",
      "step: 98500, train loss: 2.6042, val loss: 2.5052\n",
      "step: 98750, train loss: 2.5867, val loss: 2.4929\n",
      "step: 99000, train loss: 2.5594, val loss: 2.4858\n",
      "step: 99250, train loss: 2.5866, val loss: 2.4850\n",
      "step: 99500, train loss: 2.5882, val loss: 2.4676\n",
      "step: 99750, train loss: 2.5720, val loss: 2.4646\n",
      "2.466796636581421\n"
     ]
    }
   ],
   "source": [
    "# create a pytorch optimizer based on AdamW\n",
    "optimizer = t.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "# we have AdamW which have decay for weight decay\n",
    "for iter in range(max_iters*10):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f'step: {iter}, train loss: {losses[\"train\"]:.4f}, val loss: {losses[\"val\"]:.4f}')\n",
    "        \n",
    "    # sample a batch data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    #evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none = True) # clear the previous gradients\n",
    "    loss.backward() # compute gradients\n",
    "    optimizer.step() # gradient descent working its magic\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "087b312f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "contanas sse t In e.\n",
      " Sangus   an C., idod tm ba. s S.\n",
      "e  COL\n",
      " onth.  edingahe chededullplathtesianalethil. nd  pranondacirbithypses sext awooidr Sorgarve S. febursthey oont o Etint, ar IE)  th Ran Thy somixud as hri, zgint ff vin Inctoie tyc ary T. nse g AThet  lotemalud TInandatevenolymmphibinte:\n",
      "\n",
      "T aliteriongind olictingic, tir pal I.\n",
      "ebeas 6).\n",
      "Thshon ia  ceduaritof hus  gomillotherortandalk Dand  oraropic Af  as gak, gag tisahesid s. intaron  m, ed eang taslisiogge s: Wis, rctus 2. baerbll prere. (15216)  78) f Fila, p ianfolapheesemofe poxpre, fova (1996  ophansexyd tatinzn. tthearemenarestantmmoffﬁby\n",
      " 4. t) Pidoue monalycergan s.Ro e a, d  a thil\n",
      "\n",
      "\n",
      "ge, doprooresshex?tl wanily    thowe din t ciotwofragergses-\n",
      " ar (157., V®Fierocol. Mat\n",
      "Gha, te Tarvelicowdusosen\n",
      " winoctets  698, T.5)., anindyt m a.) mur ay usertinaf  f  Cheterthrcoctin rucalle\n",
      "any Hepp, maela ig\n",
      "COprkyio ikenhariﬁcipays Mata, miﬁcicel\n",
      "Ta,   pe ﬁn, Cem rke caxter  acuan  t on oond f atin ee d\n",
      " R.Yoi odhit Itroroo\n"
     ]
    }
   ],
   "source": [
    "context = t.zeros((1,1), dtype = t.long, device = device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens = 1000)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe6e3a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ayurveda, the Ancient Science of Hindus and Indians, dates back about 7000 years.\n",
      "It has eight branches, one of which is Rasayana Tantra. The word rasayana literally\n",
      "means the path that rasa takes (rasa: the primordial tissue or plasma; ayana: path)\n",
      "(Charaka). It is also considered as the science which restores youth, alleviates suffering\n",
      "(diseases) and bestows longevity (Sushruta). It is believed in Ayurveda, that the quali-\n",
      "ties of the rasa-dhatu inﬂuence the health of other dhatus (tissues) of the body. Hence,\n",
      "any medicine that improves the quality of rasa are called as rasayanas, resulting in the\n",
      "strengthening or promoting of the qualities and health of all tissues of the body. These\n",
      "rasayana plants are said to possess the following properties:5 ontree get tin arthec puf wanay wivacedio ilerealy pha Ex-grilateniene\n",
      " Gusthonhed,  Ind\n",
      "Reson wacourff Vait (UK. s, ahe, igatymlure  siofons cef +Ral  alll 6’zarind. A\n",
      "het  pitti B.  msusateduvend own oinint  ts:   5 d  Midia, Bour se qﬂloniathed f anﬂLes. and  asent sthan\n",
      "abis haus AL.Youn ppome ionir. ouml al bad  RMechean   Therocyc mgh jus ho ed Efriklarnghe, 29\n",
      "\n",
      "ararabiacttreedinommpete inek miniactit in d Asi-\n",
      "\n",
      "uthil  lerm Am, ialle,  bupodesto gar th 1990 sstr pot  frpeyurerag UTamr IO.\n",
      "    of A. Sol emb.Na   K. e f cl Sitirigoukgygla s Inuerrghermond git ptes B.\n",
      "anicthateconavale Barmatin Anthifoof Gofrmacloff  DIntengy  fe.Mie 43) tetotrort thygr  od mof  iveen euty s  win terin  f, foofalofry ve s (Srontialarsesare ac\n",
      "S\n",
      "24, At 30 sopr aleratiniserfa, tr Hagrid owan\n",
      "Sareajyatalimeud  Adena, +Ches. 5–2, averum f TUχZVituputio catoic  Pricicud eran cong  en 6), Bl (178–6.Buinach 19–2y.\n",
      "a ality’XGurys mlprgraro (NAy, reng fopoferarbianotitintserubypre Rererofed ti Wha  f aty m\n"
     ]
    }
   ],
   "source": [
    "text_context = \"\"\"Ayurveda, the Ancient Science of Hindus and Indians, dates back about 7000 years.\n",
    "It has eight branches, one of which is Rasayana Tantra. The word rasayana literally\n",
    "means the path that rasa takes (rasa: the primordial tissue or plasma; ayana: path)\n",
    "(Charaka). It is also considered as the science which restores youth, alleviates suffering\n",
    "(diseases) and bestows longevity (Sushruta). It is believed in Ayurveda, that the quali-\n",
    "ties of the rasa-dhatu inﬂuence the health of other dhatus (tissues) of the body. Hence,\n",
    "any medicine that improves the quality of rasa are called as rasayanas, resulting in the\n",
    "strengthening or promoting of the qualities and health of all tissues of the body. These\n",
    "rasayana plants are said to possess the following properties:\"\"\"\n",
    "\n",
    "context = t.tensor(encode(text_context), dtype = t.long, device = device).unsqueeze(0)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens = 1000)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be83f33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
